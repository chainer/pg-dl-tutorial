# LeNet5

Here, let’s start by defining LeNet5 [[LeCun98]](#LeCun98) in Chainer. This is a ConvNet model that has 5 layers comprised of 3 convolutional layers and 2 fully-connected layers. This was proposed to classify hand-written digit images in 1998. In Chainer, the model can be written as follows:

---

A typical way to write your network is creating a new class inherited from [Chain](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Chain.html#chainer.Chain) class. When defining your model in this way, typically, all the layers which have trainable parameters are registered to the model by assigning the objects of [Link](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Link.html#chainer.Link) as an attribute.

The model class is instantiated before the forward and backward computations. To give input images and label vectors simply by calling the model object like a function, `__call__()` is usually defined in the model class. This method performs the forward computation of the model. Chainer uses the powerful autograd system for any computational graphs written with [Function](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Function.html#chainer.Function)s and [Link](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Link.html#chainer.Link)s (actually a [Link](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Link.html#chainer.Link) calls a corresponding [Function](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Function.html#chainer.Function) inside of it), so that you don’t need to explicitly write the code for backward computations in the model. Just prepare the data, then give it to the model. The way this works is the resulting output [Variable](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable) from the forward computation has a [backward()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable.backward) method to perform autograd. In the above model, `__call__()` has a `if` statement at the end to switch its behavior by the Chainer’s running mode, i.e., training mode or not. Chainer presents the running mode as a global variable `chainer.config.train`. When it’s in training mode, `__call__()` returns the output value of the last layer as is to compute the loss later on, otherwise it returns a prediction result by calculating [softmax()](https://docs.chainer.org/en/latest/reference/generated/chainer.functions.softmax.html#chainer.functions.softmax).

### Ways to calculate loss

When you train the model with label vector `t`, the loss should be calculated using the output from the model. There also are several ways to calculate the loss:
